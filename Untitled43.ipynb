{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled43.ipynb",
      "provenance": [],
      "mount_file_id": "180nmZwcBzuNlM1oqe5Tojx_JVeP6VdMj",
      "authorship_tag": "ABX9TyONujAjFdYp6kXR0SsI7WEV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Varun-Mulchandani/roBERTa_based_SQuAD_QA/blob/master/Untitled43.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYUgSYyy7bBN",
        "colab_type": "text"
      },
      "source": [
        "Confirming GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNdP3OR65Hbh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cde6ae71-d03b-4247-f280-07efd09e9ed7"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stYUaQM07gmm",
        "colab_type": "text"
      },
      "source": [
        "For this project, I will be using HuggingFace's transformers library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HswZUov85Vyh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "981103c1-d60e-4b4d-ecd0-5babde44ac29"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/38/c9527aa055241c66c4d785381eaf6f80a28c224cae97daa1f8b183b5fabb/transformers-2.9.0-py3-none-any.whl (635kB)\n",
            "\r\u001b[K     |▌                               | 10kB 15.9MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 4.3MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 5.6MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 5.6MB/s eta 0:00:01\r\u001b[K     |██▋                             | 51kB 4.8MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 5.3MB/s eta 0:00:01\r\u001b[K     |███▋                            | 71kB 5.5MB/s eta 0:00:01\r\u001b[K     |████▏                           | 81kB 5.6MB/s eta 0:00:01\r\u001b[K     |████▋                           | 92kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 102kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 112kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 122kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 133kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 143kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 153kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 163kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 174kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 184kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 194kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 204kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 215kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 225kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 235kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 245kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 256kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 266kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 276kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 286kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 296kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 307kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████                | 317kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 327kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 337kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 348kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 358kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 368kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 378kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 389kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 399kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 409kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 419kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 430kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 440kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 450kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 460kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 471kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 481kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 491kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 501kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 512kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 522kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 532kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 542kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 552kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 563kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 573kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 583kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 593kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 604kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 614kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 624kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 634kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 645kB 5.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 16.8MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 37.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 40.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=1b2c97473ad27449d38b556597f4f772cef5d92667ab3b295c5810a302e94247\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.86 tokenizers-0.7.0 transformers-2.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOGCPDBz7mt_",
        "colab_type": "text"
      },
      "source": [
        "Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5zlrgIl5i3q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "72b878ad-5922-4ae0-9cef-185d230f51ca"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from transformers import *\n",
        "import tokenizers\n",
        "print('TF version', tf.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF version 2.2.0-rc4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcK0KYsh7qmN",
        "colab_type": "text"
      },
      "source": [
        "Max length for the input sequence is reduced to 384 due to lack of computaitonal resources for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDN0CKbE5ztn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 384\n",
        "\n",
        "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
        "    vocab_file = 'drive/My Drive/vocab-roberta-base.json',\n",
        "    merges_file = 'drive/My Drive/merges-roberta-base.txt',\n",
        "    lowercase = True,\n",
        "    add_prefix_space = True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjXKqzG48KvL",
        "colab_type": "text"
      },
      "source": [
        "Loading the SQuAD dataset which has been converted to a csv file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r197F3Uo6fyx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "outputId": "fb7549ae-6e1a-4eef-dea2-f3e9281bb49d"
      },
      "source": [
        "train = pd.read_csv('drive/My Drive/train (3).csv').fillna('')\n",
        "train.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>title</th>\n",
              "      <th>question</th>\n",
              "      <th>id</th>\n",
              "      <th>answers</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>When did Beyonce start becoming popular?</td>\n",
              "      <td>56be85543aeaaa14008c9063</td>\n",
              "      <td>in the late 1990s</td>\n",
              "      <td>269</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>What areas did Beyonce compete in when she was...</td>\n",
              "      <td>56be85543aeaaa14008c9065</td>\n",
              "      <td>singing and dancing</td>\n",
              "      <td>207</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
              "      <td>56be85543aeaaa14008c9066</td>\n",
              "      <td>2003</td>\n",
              "      <td>526</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>In what city and state did Beyonce  grow up?</td>\n",
              "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
              "      <td>Houston, Texas</td>\n",
              "      <td>166</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>In which decade did Beyonce become famous?</td>\n",
              "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
              "      <td>late 1990s</td>\n",
              "      <td>276</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                            context\n",
              "0           0  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n",
              "1           1  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n",
              "2           2  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n",
              "3           3  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n",
              "4           4  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7xIamCY6n82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rec = train.shape[0]  # Number of records in the training set\n",
        "inputs = np.ones((rec, MAX_LEN), dtype = 'int32') # Input vector\n",
        "attention_mask = np.zeros((rec, MAX_LEN), dtype = 'int32') # Attention Mask\n",
        "token_type_ids = np.zeros((rec, MAX_LEN), dtype = 'int32') # Tokens produced\n",
        "start_tokens = np.zeros((rec, MAX_LEN), dtype = 'int32') # Start logit for answer\n",
        "end_tokens = np.zeros((rec, MAX_LEN), dtype = 'int32') # End logit for answer\n",
        "\n",
        "for i in range(rec):\n",
        "\n",
        "  context = ' '+' '.join(train.loc[i, 'context'].split())\n",
        "  answer = ' '+' '.join(train.loc[i, 'answers'].split())\n",
        "  question = ' '+' '.join(train.loc[i, 'question'].split())\n",
        "\n",
        "  start_idx = train.loc[i, 'answer_start']\n",
        "\n",
        "  chars = np.zeros((len(context)))\n",
        "  chars[start_idx:start_idx + len(answer)] = 1\n",
        "  if context[start_idx - 1] == ' ':\n",
        "    chars[start_idx - 1] = 1\n",
        "  \n",
        "  enc1 = tokenizer.encode(context)\n",
        "  enc2 = tokenizer.encode(question)\n",
        "\n",
        "  # For resource limitations only.\n",
        "\n",
        "  if len(enc1) + len(enc2) + 4 < MAX_LEN:\n",
        "\n",
        "    #creating offsets\n",
        "    offsets = []\n",
        "    start_idx = 0\n",
        "\n",
        "    for t in enc1.ids:\n",
        "      w = tokenizer.decode([t])\n",
        "      offsets.append((start_idx, start_idx + len(w)))\n",
        "      start_idx += len(w)\n",
        "    \n",
        "    tokens = []\n",
        "    for j, (a, b) in enumerate(offsets):\n",
        "      sum_ = np.sum(chars[a:b])\n",
        "      if sum_ > 0:\n",
        "        tokens.append(j)\n",
        "\n",
        "    # The input for roberta is in the form <s> Question </s></s> Answer </s>\n",
        "    \n",
        "    inputs[i, :len(enc1.ids) + len(enc2.ids) + 4] = [0] + enc2.ids + [2,2] + enc1.ids + [2]\n",
        "\n",
        "    attention_mask[i, :len(enc1.ids) + len(enc2.ids) + 4] = 1\n",
        "\n",
        "    if len(tokens) > 0:\n",
        "      start_tokens[i, tokens[0] + 1] = 1\n",
        "      end_tokens[i, tokens[-1] + 1] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWJ6yUM3FxIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  ids = tf.keras.layers.Input((MAX_LEN,), dtype = tf.int32)\n",
        "  att = tf.keras.layers.Input((MAX_LEN,), dtype = tf.int32)\n",
        "  tok = tf.keras.layers.Input((MAX_LEN,), dtype = tf.int32)\n",
        "\n",
        "  config = RobertaConfig.from_pretrained('drive/My Drive/config-roberta-base.json')\n",
        "  bert_model = TFRobertaModel.from_pretrained('drive/My Drive/pretrained-roberta-base.h5', config = config)\n",
        "  x = bert_model(ids, attention_mask=att, token_type_ids = tok)\n",
        "\n",
        "  # For start logit\n",
        "\n",
        "  x1 = tf.keras.layers.Dropout(0.1)(x[0])\n",
        "  x1 = tf.keras.layers.Conv1D(1,1)(x1)\n",
        "  x1 = tf.keras.layers.Flatten()(x1)\n",
        "  x1 = tf.keras.layers.Activation('softmax')(x1)\n",
        "\n",
        "  # For end logit\n",
        "\n",
        "  x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
        "  x2 = tf.keras.layers.Conv1D(1,1)(x2)\n",
        "  x2 = tf.keras.layers.Flatten()(x2)\n",
        "  x2 = tf.keras.layers.Activation('softmax')(x2)\n",
        "\n",
        "  # Initalising the model\n",
        "\n",
        "  model = tf.keras.models.Model(inputs = [ids, att, tok], outputs = [x1, x2])\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate = 3e-5)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer = optimizer)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oE0rxYcoH5uz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IeU7b3IH8mQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "923ba8a0-1443-4615-a66f-d85c84743793"
      },
      "source": [
        "history = model.fit([inputs, attention_mask, token_type_ids], [start_tokens, end_tokens], epochs = 1, batch_size = 4, validation_split = 0.1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "11924/19535 [=================>............] - ETA: 1:31:36 - loss: 6.2605 - activation_7_loss: 3.1029 - activation_8_loss: 3.1577"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzFyYqurIO-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}