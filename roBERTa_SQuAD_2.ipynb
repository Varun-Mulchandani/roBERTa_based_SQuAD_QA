{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "roBERTa_SQuAD_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "180nmZwcBzuNlM1oqe5Tojx_JVeP6VdMj",
      "authorship_tag": "ABX9TyNIfF1Qk1/b3geLUZJVbMnr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Varun-Mulchandani/roBERTa_based_SQuAD_QA/blob/master/roBERTa_SQuAD_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYUgSYyy7bBN",
        "colab_type": "text"
      },
      "source": [
        "Confirming GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNdP3OR65Hbh",
        "colab_type": "code",
        "outputId": "8818e1cc-9dc6-4aa4-e7d4-876e79dc7c03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stYUaQM07gmm",
        "colab_type": "text"
      },
      "source": [
        "For this project, I will be using HuggingFace's transformers library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HswZUov85Vyh",
        "colab_type": "code",
        "outputId": "7df52793-e03f-4767-b850-dc07b9c805ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/38/c9527aa055241c66c4d785381eaf6f80a28c224cae97daa1f8b183b5fabb/transformers-2.9.0-py3-none-any.whl (635kB)\n",
            "\r\u001b[K     |▌                               | 10kB 23.2MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 1.5MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 2.0MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 1.6MB/s eta 0:00:01\r\u001b[K     |██▋                             | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 2.1MB/s eta 0:00:01\r\u001b[K     |███▋                            | 71kB 2.3MB/s eta 0:00:01\r\u001b[K     |████▏                           | 81kB 2.6MB/s eta 0:00:01\r\u001b[K     |████▋                           | 92kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 102kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 112kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 122kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 133kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 143kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 153kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 163kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 174kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 184kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 194kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 204kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 215kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 225kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 235kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 245kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 256kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 266kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 276kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 286kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 296kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 307kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 317kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 327kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 337kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 348kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 358kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 368kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 378kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 389kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 399kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 409kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 419kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 430kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 440kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 450kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 460kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 471kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 481kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 491kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 501kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 512kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 522kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 532kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 542kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 552kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 563kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 573kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 583kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 593kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 604kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 614kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 624kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 634kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 645kB 2.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 12.8MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 17.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 37.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=004c6765facc3daf77bf13ec04863aded1d5ab7645742935aaa4f914acaf8252\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.86 tokenizers-0.7.0 transformers-2.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOGCPDBz7mt_",
        "colab_type": "text"
      },
      "source": [
        "Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5zlrgIl5i3q",
        "colab_type": "code",
        "outputId": "4a94b893-1d5d-4a54-8a29-54637473ba0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from transformers import *\n",
        "import tokenizers\n",
        "print('TF version', tf.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF version 2.2.0-rc4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcK0KYsh7qmN",
        "colab_type": "text"
      },
      "source": [
        "Max length for the input sequence is reduced to 384 due to lack of computaitonal resources for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDN0CKbE5ztn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 384\n",
        "\n",
        "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
        "    vocab_file = 'drive/My Drive/vocab-roberta-base.json',\n",
        "    merges_file = 'drive/My Drive/merges-roberta-base.txt',\n",
        "    lowercase = True,\n",
        "    add_prefix_space = True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjXKqzG48KvL",
        "colab_type": "text"
      },
      "source": [
        "Loading the SQuAD dataset which has been converted to a csv file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r197F3Uo6fyx",
        "colab_type": "code",
        "outputId": "d9b2a33d-a260-4ea4-a2d2-c08201666a9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        }
      },
      "source": [
        "train = pd.read_csv('drive/My Drive/train (3).csv').fillna('')\n",
        "train.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>title</th>\n",
              "      <th>question</th>\n",
              "      <th>id</th>\n",
              "      <th>answers</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>When did Beyonce start becoming popular?</td>\n",
              "      <td>56be85543aeaaa14008c9063</td>\n",
              "      <td>in the late 1990s</td>\n",
              "      <td>269</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>What areas did Beyonce compete in when she was...</td>\n",
              "      <td>56be85543aeaaa14008c9065</td>\n",
              "      <td>singing and dancing</td>\n",
              "      <td>207</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
              "      <td>56be85543aeaaa14008c9066</td>\n",
              "      <td>2003</td>\n",
              "      <td>526</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>In what city and state did Beyonce  grow up?</td>\n",
              "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
              "      <td>Houston, Texas</td>\n",
              "      <td>166</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>In which decade did Beyonce become famous?</td>\n",
              "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
              "      <td>late 1990s</td>\n",
              "      <td>276</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                            context\n",
              "0           0  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n",
              "1           1  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n",
              "2           2  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n",
              "3           3  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n",
              "4           4  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7xIamCY6n82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rec = train.shape[0]  # Number of records in the training set\n",
        "inputs = np.ones((rec, MAX_LEN), dtype = 'int32') # Input vector\n",
        "attention_mask = np.zeros((rec, MAX_LEN), dtype = 'int32') # Attention Mask\n",
        "token_type_ids = np.zeros((rec, MAX_LEN), dtype = 'int32') # Tokens produced\n",
        "start_tokens = np.zeros((rec, MAX_LEN), dtype = 'int32') # Start logit for answer\n",
        "end_tokens = np.zeros((rec, MAX_LEN), dtype = 'int32') # End logit for answer\n",
        "\n",
        "for i in range(rec):\n",
        "\n",
        "  context = ' '+' '.join(train.loc[i, 'context'].split())\n",
        "  answer = ' '+' '.join(train.loc[i, 'answers'].split())\n",
        "  question = ' '+' '.join(train.loc[i, 'question'].split())\n",
        "\n",
        "  start_idx = train.loc[i, 'answer_start']\n",
        "\n",
        "  chars = np.zeros((len(context)))\n",
        "  chars[start_idx:start_idx + len(answer)] = 1\n",
        "  if context[start_idx - 1] == ' ':\n",
        "    chars[start_idx - 1] = 1\n",
        "  \n",
        "  enc1 = tokenizer.encode(context)\n",
        "  enc2 = tokenizer.encode(question)\n",
        "\n",
        "  # For resource limitations only.\n",
        "\n",
        "  if len(enc1) + len(enc2) + 4 < MAX_LEN:\n",
        "\n",
        "    #creating offsets\n",
        "    offsets = []\n",
        "    start_idx = 0\n",
        "\n",
        "    for t in enc1.ids:\n",
        "      w = tokenizer.decode([t])\n",
        "      offsets.append((start_idx, start_idx + len(w)))\n",
        "      start_idx += len(w)\n",
        "    \n",
        "    tokens = []\n",
        "    for j, (a, b) in enumerate(offsets):\n",
        "      sum_ = np.sum(chars[a:b])\n",
        "      if sum_ > 0:\n",
        "        tokens.append(j)\n",
        "\n",
        "    # The input for roberta is in the form <s> Question </s></s> Answer </s>\n",
        "    \n",
        "    inputs[i, :len(enc1.ids) + len(enc2.ids) + 4] = [0] + enc2.ids + [2,2] + enc1.ids + [2]\n",
        "\n",
        "    attention_mask[i, :len(enc1.ids) + len(enc2.ids) + 4] = 1\n",
        "\n",
        "    if len(tokens) > 0:\n",
        "      start_tokens[i, tokens[0] + 1] = 1\n",
        "      end_tokens[i, tokens[-1] + 1] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHcyvBI5KcK2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "853bc005-8a51-4a8f-a80b-52868f9620b5"
      },
      "source": [
        "tokens"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4, 5]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWJ6yUM3FxIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  ids = tf.keras.layers.Input((MAX_LEN,), dtype = tf.int32)\n",
        "  att = tf.keras.layers.Input((MAX_LEN,), dtype = tf.int32)\n",
        "  tok = tf.keras.layers.Input((MAX_LEN,), dtype = tf.int32)\n",
        "\n",
        "  config = RobertaConfig.from_pretrained('drive/My Drive/config-roberta-base.json')\n",
        "  bert_model = TFRobertaModel.from_pretrained('drive/My Drive/pretrained-roberta-base.h5', config = config)\n",
        "  x = bert_model(ids, attention_mask=att, token_type_ids = tok)\n",
        "\n",
        "  # For start logit\n",
        "\n",
        "  x1 = tf.keras.layers.Dropout(0.1)(x[0])\n",
        "  x1 = tf.keras.layers.Conv1D(1,1)(x1)\n",
        "  x1 = tf.keras.layers.Flatten()(x1)\n",
        "  x1 = tf.keras.layers.Activation('softmax')(x1)\n",
        "\n",
        "  # For end logit\n",
        "\n",
        "  x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
        "  x2 = tf.keras.layers.Conv1D(1,1)(x2)\n",
        "  x2 = tf.keras.layers.Flatten()(x2)\n",
        "  x2 = tf.keras.layers.Activation('softmax')(x2)\n",
        "\n",
        "  # Initalising the model\n",
        "\n",
        "  model = tf.keras.models.Model(inputs = [ids, att, tok], outputs = [x1, x2])\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate = 3e-5)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer = optimizer)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oE0rxYcoH5uz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IeU7b3IH8mQ",
        "colab_type": "code",
        "outputId": "7cf7695a-b130-441c-a448-966049f3e5fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "history = model.fit([inputs, attention_mask, token_type_ids], [start_tokens, end_tokens], epochs = 1, batch_size = 4, validation_split = 0.1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "19535/19535 [==============================] - 5016s 257ms/step - loss: 5.4522 - activation_7_loss: 2.7096 - activation_8_loss: 2.7427 - val_loss: 3.4484 - val_activation_7_loss: 1.7218 - val_activation_8_loss: 1.7265\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFuDn_hUCzY-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "cf1e193f-0149-427f-9820-cf08d1b0c3f2"
      },
      "source": [
        "print(enc1.ids)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[449, 2681, 14785, 257, 20887, 343, 36, 7203, 438, 238, 11, 645, 7, 3720, 758, 3115, 34, 2885, 41, 758, 3115, 3556, 18014, 36, 21163, 322, 6301, 438, 18, 78, 758, 1291, 21, 2885, 11, 14873, 19, 5, 343, 9, 364, 3252, 2552, 6, 1021, 46670, 6, 10409, 982, 4, 42, 1940, 34, 57, 617, 9094, 30, 10584, 4828, 4158, 19, 290, 97, 1947, 35, 475, 5992, 783, 7087, 343, 9, 1236, 20948, 6, 4533, 17419, 9, 5, 201, 102, 6, 1423, 1097, 261, 36, 30520, 18780, 3863, 43, 9, 127, 43918, 6, 3023, 118, 108, 260, 9, 5, 82, 18, 16441, 9, 1855, 1243, 6, 5251, 7771, 9, 12138, 42292, 6, 8, 19290, 46125, 9, 5, 7368, 16441, 9, 449, 33594, 4, 6301, 438, 18, 5891, 25541, 16, 7, 6292, 63, 10405, 19, 2241, 9636, 749, 6, 97, 758, 2244, 8, 171, 97, 538, 1947, 9, 5, 232, 7, 3042, 357, 4879, 1052, 8, 18477, 1767, 13, 449, 2681, 14785, 257, 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3M2SP0rNnO0L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "455ecba0-effa-47d3-e949-0a9d504e7957"
      },
      "source": [
        "inp = 'Google LLC is an American multinational technology company that specializes in Internet-related services and products, which include online advertising technologies, search engine, cloud computing, software, and hardware. It is considered one of the Big Four technology companies, alongside Amazon, Apple, and Facebook.'\n",
        "que = 'What is Google?'\n",
        "inp_id = np.zeros((1,MAX_LEN),dtype='int32')\n",
        "attn_mask_input = np.zeros((1,MAX_LEN),dtype='int32')\n",
        "token_type_id_input = np.zeros((1,MAX_LEN),dtype='int32')\n",
        "inpenc = tokenizer.encode(inp)\n",
        "queenc = tokenizer.encode(que)\n",
        "print(len(que))\n",
        "#chars_1 = np.zeros((len(inp)))\n",
        "#chars_1[idx:idx + len(text2)] = 1\n",
        "\n",
        "offset = []\n",
        "id_ = 0\n",
        "for t in inpenc.ids:\n",
        "  w = tokenizer.decode([t])\n",
        "  offset.append((id_, id_ + len(w)))\n",
        "  id_ += len(w)\n",
        "print(offset)\n",
        "\n",
        "inp_id[0,:len(inpenc.ids)+len(queenc.ids) + 4] = [0] + queenc.ids + [2,2] + inpenc.ids + [2]\n",
        "attn_mask_input[0,:len(inpenc.ids)+len(queenc.ids) + 4] = 1"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15\n",
            "[(0, 7), (7, 10), (10, 11), (11, 14), (14, 17), (17, 24), (24, 26), (26, 40), (40, 51), (51, 59), (59, 64), (64, 76), (76, 79), (79, 88), (88, 89), (89, 96), (96, 105), (105, 109), (109, 118), (118, 119), (119, 125), (125, 133), (133, 140), (140, 152), (152, 165), (165, 166), (166, 173), (173, 180), (180, 181), (181, 187), (187, 197), (197, 198), (198, 207), (207, 208), (208, 212), (212, 221), (221, 222), (222, 225), (225, 228), (228, 239), (239, 243), (243, 246), (246, 250), (250, 254), (254, 259), (259, 270), (270, 280), (280, 281), (281, 291), (291, 294), (294, 298), (298, 299), (299, 305), (305, 306), (306, 310), (310, 319), (319, 320)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxdQkermPngl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_ans(inp,que,model,tokenizer):\n",
        "  inp_id = np.zeros((1,MAX_LEN),dtype='int32')\n",
        "  attn_mask_input = np.zeros((1,MAX_LEN),dtype='int32')\n",
        "  token_type_id_input = np.zeros((1,MAX_LEN),dtype='int32')\n",
        "  inpenc = tokenizer.encode(inp)\n",
        "  queenc = tokenizer.encode(que)\n",
        "  inp_id[0,:len(inpenc.ids)+len(queenc.ids) + 4] = [0] + queenc.ids + [2,2] + inpenc.ids + [2]\n",
        "  attn_mask_input[0,:len(inpenc.ids)+len(queenc.ids) + 4] = 1\n",
        "  s, f = model.predict([inp_id,attn_mask_input,token_type_id_input])\n",
        "  s_ = np.argmax(s[0,])\n",
        "  f_ = np.argmax(f[0,])\n",
        "  ans = tokenizer.decode(inpenc.ids[s_ - 1: f_ + 1])\n",
        "\n",
        "  return ans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsD8JPd2RNlN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "80db94bb-d7bb-42fd-f156-7695ceabaf09"
      },
      "source": [
        "inp = 'Google LLC is an American multinational technology company that specializes in Internet-related services and products, which include online advertising technologies, search engine, cloud computing, software, and hardware. It is considered one of the Big Four technology companies, alongside Amazon, Apple, and Facebook.'\n",
        "que = 'What is Google?'\n",
        "ans = generate_ans(inp,que,model,tokenizer)\n",
        "print(ans)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " google llc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJJAZKHMXFG8",
        "colab_type": "text"
      },
      "source": [
        "Evidently, this model required more training and fine tuning (loss alone was pretty big.). In the next iteration, testing metric and more training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zu3gPKzyKCiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}